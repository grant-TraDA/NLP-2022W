
%Lime
@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}
%SHAP
@article{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
%1980 paper about vocabulary
@article{hopper1980transitivity,
  title={Transitivity in grammar and discourse},
  author={Hopper, Paul J and Thompson, Sandra A},
  journal={language},
  pages={251--299},
  year={1980},
  publisher={JSTOR}
}
@inproceedings{sun2019fine,
  title={How to fine-tune bert for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={China national conference on Chinese computational linguistics},
  pages={194--206},
  year={2019},
  organization={Springer}
}


@article{abbott2020can,
  title={Can Spoilers in Online Reviews Impact Viewer Enjoyment?},
  author={Abbott, Marshall},
  year={2020}
}


@article{li2022exploring,
  title={Exploring the spoiler effect in the digital age: Evidence from the movie industry},
  author={Li, Yang and Luo, Xin Robert and Li, Kai and Xu, Xiaobo},
  journal={Decision Support Systems},
  volume={157},
  pages={113755},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{guo2010finding,
  title={Finding the storyteller: automatic spoiler tagging using linguistic cues},
  author={Guo, Sheng and Ramakrishnan, Naren},
  booktitle={Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)},
  pages={412--420},
  year={2010}
}


@misc{enam_biswas_2021, 
title={IMDb Review Dataset - ebD}, 
url={https://www.kaggle.com/dsv/1836923}, 
howpublished = "\url{https://www.kaggle.com/dsv/1836923}",
DOI={10.34740/KAGGLE/DSV/1836923}, 
publisher={Kaggle}, 
author={Enam Biswas}, 
year={2021} 
}
@inproceedings{chang2018deep,
  title={A deep neural spoiler detection model using a genre-aware attention mechanism},
  author={Chang, Buru and Kim, Hyunjae and Kim, Raehyun and Kim, Deahan and Kang, Jaewoo},
  booktitle={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  pages={183--195},
  year={2018},
  organization={Springer}
}

@inproceedings{nakamura2007temporal,
  title={Temporal filtering system to reduce the risk of spoiling a user's enjoyment},
  author={Nakamura, Satoshi and Tanaka, Katsumi},
  booktitle={Proceedings of the 12th international conference on Intelligent user interfaces},
  pages={345--348},
  year={2007}
}

@inproceedings{lime,
  author    = {Marco Tulio Ribeiro and
               Sameer Singh and
               Carlos Guestrin},
  title     = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on
               Knowledge Discovery and Data Mining, San Francisco, CA, USA, August
               13-17, 2016},
  pages     = {1135--1144},
  year      = {2016},
}
@incollection{NIPS2017_7062,
title = {A Unified Approach to Interpreting Model Predictions},
author = {Lundberg, Scott M and Lee, Su-In},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4765--4774},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}
@article{deyoung2019eraser,
  title={ERASER: A benchmark to evaluate rationalized NLP models},
  author={DeYoung, Jay and Jain, Sarthak and Rajani, Nazneen Fatema and Lehman, Eric and Xiong, Caiming and Socher, Richard and Wallace, Byron C},
  journal={arXiv preprint arXiv:1911.03429},
  year={2019}
}
@article{boyd2013spoiler,
  title={Spoiler alert: Machine learning approaches to detect social media posts with revelatory information},
  author={Boyd-Graber, Jordan and Glasgow, Kimberly and Zajac, Jackie Sauter},
  journal={Proceedings of the American Society for Information Science and Technology},
  volume={50},
  number={1},
  pages={1--9},
  year={2013},
  publisher={Wiley Online Library}
}

@incollection{torrey2010transfer,
  title={Transfer learning},
  author={Torrey, Lisa and Shavlik, Jude},
  booktitle={Handbook of research on machine learning applications and trends: algorithms, methods, and techniques},
  pages={242--264},
  year={2010},
  publisher={IGI global}
}

@article{weiss2016survey,
  title={A survey of transfer learning},
  author={Weiss, Karl and Khoshgoftaar, Taghi M and Wang, DingDing},
  journal={Journal of Big data},
  volume={3},
  number={1},
  pages={1--40},
  year={2016},
  publisher={SpringerOpen}
}


@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}


@inproceedings{yang2016hierarchical,
  title={Hierarchical attention networks for document classification},
  author={Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  booktitle={Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies},
  pages={1480--1489},
  year={2016}
}


@article{wroblewska2021spoiler,
  title={Spoiler in a Textstack: How Much Can Transformers Help?},
  author={Wr{\'o}blewska, Anna and Rzepi{\'n}ski, Pawe{\l} and Sysko-Roma{\'n}czuk, Sylwia},
  journal={arXiv preprint arXiv:2112.12913},
  year={2021}
}
@inproceedings{golbeck2012twitter,
  title={The twitter mute button: a web filtering challenge},
  author={Golbeck, Jennifer},
  booktitle={Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages={2755--2758},
  year={2012}
}
@article{wan2019fine,
  title={Fine-grained spoiler detection from large-scale review corpora},
  author={Wan, Mengting and Misra, Rishabh and Nakashole, Ndapa and McAuley, Julian},
  journal={arXiv preprint arXiv:1905.13416},
  year={2019}
}
%Lime
@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}
%SHAP
@article{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
%1980 paper about vocabulary
@article{hopper1980transitivity,
  title={Transitivity in grammar and discourse},
  author={Hopper, Paul J and Thompson, Sandra A},
  journal={language},
  pages={251--299},
  year={1980},
  publisher={JSTOR}
}
@inproceedings{sun2019fine,
  title={How to fine-tune bert for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={China national conference on Chinese computational linguistics},
  pages={194--206},
  year={2019},
  organization={Springer}
}

@misc{enam_biswas_2021, 
title={IMDb Review Dataset - ebD}, 
url={https://www.kaggle.com/dsv/1836923}, 
howpublished = "\url{https://www.kaggle.com/dsv/1836923}",
DOI={10.34740/KAGGLE/DSV/1836923}, 
publisher={Kaggle}, 
author={Enam Biswas}, 
year={2021} 
}
@misc{medium_extract, 
title={Extract the right Phrase From Sentence}, 
url={https://medium.com/analytics-vidhya/extract-the-right-phrase-from-sentence-29aa5f8b9182}, 
howpublished = "\url{https://medium.com/analytics-vidhya/extract-the-right-phrase-from-sentence-29aa5f8b9182}",
publisher={Kaggle}, 
author={Jitendra Dash}, 
year={2021} 
}
@misc{kaggle_comp,
    author = {Maggie, Phil Culliton, Wei Chen},
    title = {Tweet Sentiment Extraction},
    publisher = {Kaggle},
    year = {2020},
    howpublished = "\url{https://kaggle.com/competitions/tweet-sentiment-extraction}"
}
@inproceedings{lime,
  author    = {Marco Tulio Ribeiro and
               Sameer Singh and
               Carlos Guestrin},
  title     = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on
               Knowledge Discovery and Data Mining, San Francisco, CA, USA, August
               13-17, 2016},
  pages     = {1135--1144},
  year      = {2016},
}
@incollection{NIPS2017_7062,
title = {A Unified Approach to Interpreting Model Predictions},
author = {Lundberg, Scott M and Lee, Su-In},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4765--4774},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}
@article{deyoung2019eraser,
  title={ERASER: A benchmark to evaluate rationalized NLP models},
  author={DeYoung, Jay and Jain, Sarthak and Rajani, Nazneen Fatema and Lehman, Eric and Xiong, Caiming and Socher, Richard and Wallace, Byron C},
  journal={arXiv preprint arXiv:1911.03429},
  year={2019}}


@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}



@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@article{madasu2020efficient,
  title={Efficient feature selection techniques for sentiment analysis},
  author={Madasu, Avinash and Elango, Sivasankar},
  journal={Multimedia Tools and Applications},
  volume={79},
  number={9},
  pages={6313--6335},
  year={2020},
  publisher={Springer}
}
@article{jianqiang2018deep,
  title={Deep convolution neural networks for twitter sentiment analysis},
  author={Jianqiang, Zhao and Xiaolin, Gui and Xuejun, Zhang},
  journal={IEEE access},
  volume={6},
  pages={23253--23260},
  year={2018},
  publisher={IEEE}
}
@article{rezaeinia2017improving,
  title={Improving the accuracy of pre-trained word embeddings for sentiment analysis},
  author={Rezaeinia, Seyed Mahdi and Ghodsi, Ali and Rahmani, Rouhollah},
  journal={arXiv preprint arXiv:1711.08609},
  year={2017}
}
@inproceedings{zhang2011extracting,
  title={Extracting resource terms for sentiment analysis},
  author={Zhang, Lei and Liu, Bing},
  booktitle={Proceedings of 5th International Joint Conference on Natural Language Processing},
  pages={1171--1179},
  year={2011}
}
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of naacL-HLT},
  pages={4171--4186},
  year={2019}
}


@inproceedings{abadi2016tensorflow,
  title={$\{$TensorFlow$\}$: a system for $\{$Large-Scale$\}$ machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  pages={265--283},
  year={2016}
}
